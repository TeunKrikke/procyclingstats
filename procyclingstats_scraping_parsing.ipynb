{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import simplejson as json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import unidecode\n",
    "import datetime as dt\n",
    "from collections import Counter\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rider profile URLs from select races' startlists of last 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "\n",
    "def get_rider_urls(race_url,start_year,years_back):\n",
    "    base_url = 'https://www.procyclingstats.com/race/' + str(race_url) + '/'\n",
    "    print(str(race_url).upper(),'starting...')\n",
    "    for year in range(int(start_year),int(start_year-(years_back+1)),-1):\n",
    "        url = base_url + str(year) + '/startlist'\n",
    "        response = requests.get(url, headers=user_agent)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            page = response.text\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "            startlist = soup.find_all('a', class_='rider blue ')\n",
    "            urls = [x['href'] for x in startlist]\n",
    "            urls = ['https://www.procyclingstats.com/' + u for u in urls]\n",
    "            file = str(race_url) + '_' + str(year) + '.json'\n",
    "            with open(file, 'w') as f:\n",
    "                json.dump(urls, f)\n",
    "            timer = 2 + 2 * random.random()\n",
    "            print(year,'done, sleeping for',np.round(timer,2),'sec')\n",
    "            time.sleep(timer)\n",
    "        else:\n",
    "            print('unsuccessful request!')\n",
    "            print('status code:',response.status_code)\n",
    "            break\n",
    "    print('finished!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**races chosen for scraping startlists:**\n",
    "- 2007 to present\n",
    "  - giro-d-italia\n",
    "  - tour-de-france\n",
    "- 2010 to present\n",
    "  - vuelta-a-espana\n",
    "- 2013 to present\n",
    "  - strade-bianchi\n",
    "  - paris-nice\n",
    "  - milano-sanremo\n",
    "  - gent-wevelgem\n",
    "  - ronde-van-vlaanderen\n",
    "  - paris-roubaix\n",
    "  - amstel-gold-race\n",
    "  - tour-of-california\n",
    "  - liege-bastogne-liege\n",
    "  - tour-de-suisse\n",
    "  - il-lombardia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for race in races:\n",
    "    get_rider_urls(race,2017,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how many unique riders there are across those races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique rider urls: 1731\n"
     ]
    }
   ],
   "source": [
    "path = 'startlists/'\n",
    "unique_riders = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(path, '*.json')):\n",
    "    with open(filename) as f:\n",
    "        riders = json.load(f)\n",
    "        unique_riders.extend(riders)\n",
    "\n",
    "print('number of unique rider urls:',len(set(unique_riders)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.procyclingstats.com/rider/alexander-edmondson', 'https://www.procyclingstats.com/rider/francois-parisien', 'https://www.procyclingstats.com/rider/valentin-baillifard', 'https://www.procyclingstats.com/rider/adrian-saez-de-arregi', 'https://www.procyclingstats.com/rider/alex-cano-ardila', 'https://www.procyclingstats.com/rider/steve-zampieri', 'https://www.procyclingstats.com/rider/jay-robert-thomson', 'https://www.procyclingstats.com/rider/anthony-perez', 'https://www.procyclingstats.com/rider/kevin-hulsmans', 'https://www.procyclingstats.com/rider/ryder-hesjedal']\n"
     ]
    }
   ],
   "source": [
    "unique_riders = list(set(unique_riders))\n",
    "print(unique_riders[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all season pages for each rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-28 16:14:02.829642 Downloading web pages...\n",
      "2018-01-28 16:46:08.609204 Done] S[7] Saved: tim-ariesen_2012.html.htmltml_2009.html\n"
     ]
    }
   ],
   "source": [
    "start = 1681\n",
    "end = 1731\n",
    "\n",
    "print(dt.datetime.now(),'Downloading web pages...')\n",
    "for rdx, first_url in enumerate(unique_riders[start:end]):\n",
    "    # save first season and get list of remaining ones\n",
    "    response = requests.get(first_url, headers=user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    seasons = [year.text for year in soup.find('ul', class_='horiztree').contents]\n",
    "    save_name = first_url.split('/')[4] + '_' + seasons[0] + '.html'\n",
    "    rdx_str = 'R[' + str(rdx+1) + ']'\n",
    "    sdx_str = 'S[1]'\n",
    "    \n",
    "    with open(os.path.join('riders/',save_name), 'w') as file:\n",
    "        file.write(str(soup))\n",
    "    print(dt.datetime.now(),rdx_str,sdx_str,'Saved:',save_name,end='\\r')\n",
    "    \n",
    "    random_wait = 2 + 2 * random.random()\n",
    "    time.sleep(random_wait)\n",
    "    \n",
    "    # save the remaining seasons\n",
    "    if len(seasons) > 1:\n",
    "        remaining_seasons = seasons[1:len(seasons)]\n",
    "        for sdx, year in enumerate(remaining_seasons):\n",
    "            next_url = first_url + '&season=' + year\n",
    "            response = requests.get(next_url, headers=user_agent)\n",
    "            page = response.text\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "            save_name = save_name.split('_')[0] + '_' + year + '.html'\n",
    "            sdx_str = 'S[' + str(sdx+2) + ']'\n",
    "            \n",
    "            with open(os.path.join('riders/',save_name), 'w') as file:\n",
    "                file.write(str(soup))\n",
    "            print(dt.datetime.now(),rdx_str,sdx_str,'Saved:',save_name,end='\\r')\n",
    "            \n",
    "            random_wait = 2 + 2 * random.random()\n",
    "            time.sleep(random_wait)\n",
    "    else:\n",
    "        continue\n",
    "print(dt.datetime.now(),'Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**estimates after first hour:**\n",
    "- 15 pages per minute\n",
    "- 10 pages per rider\n",
    "- 1731 riders\n",
    "- 17,310 total pages\n",
    "- 900 pages per hour\n",
    "- 19 hours total scraping time\n",
    "- 16 pages per MB, total 1 GB\n",
    "\n",
    "**final tally:**\n",
    "- 17,806 pages\n",
    "- 1.14 GB\n",
    "\n",
    "**other races to consider if more riders are needed:**\n",
    "- Vuelta a San Juan (Argentina)\n",
    "- Tour Down Under (Australia)\n",
    "- Dubai Tour (United Arab Emirates)\n",
    "- Tirreno-Adriatico (Italy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download annual PCS ranking pages for all riders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "start_page = 'https://www.procyclingstats.com/rankings/me/pcs/individual'\n",
    "\n",
    "response = requests.get(start_page, headers=user_agent)\n",
    "page = response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "date_values = [item.text for item in soup.find_all('option', {'value' : re.compile('rankings.*\\/[0-9]{4}-[0-9]{2}-[0-9]{2}')})]\n",
    "date_values = [date for date in date_values if '-12-31' in date or '2018' in date]\n",
    "page_values = [item['value'] for item in soup.find('select', {'name':'page', 'style':'padding: 1px; width: 204px;'}).find_all('option', {'value' : re.compile('^[0-9]{1,2}$')})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual ranking dates: ['2018-01-30', '2016-12-31', '2015-12-31', '2014-12-31', '2013-12-31', '2012-12-31', '2011-12-31', '2010-12-31', '2009-12-31', '2008-12-31', '2007-12-31', '2006-12-31', '2005-12-31']\n",
      "\n",
      "page numbers for initial ranking date: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27']\n"
     ]
    }
   ],
   "source": [
    "print('annual ranking dates:',date_values)\n",
    "print('\\npage numbers for initial ranking date:',page_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!9 17:12:01.849824 Saved: pcs_ranking_2005-12-31_12.html\n"
     ]
    }
   ],
   "source": [
    "for date in date_values:\n",
    "    save_url = start_page + '/' + date\n",
    "    save_name = 'pcs_ranking_' + date + '_1.html'\n",
    "    response = requests.get(save_url, headers=user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    with open(os.path.join('rankings/',save_name), 'w') as file:\n",
    "        file.write(str(soup))\n",
    "    print(dt.datetime.now(),'Saved:',save_name,end='\\r')\n",
    "    \n",
    "    random_wait = 2 + 1 * random.random()\n",
    "    time.sleep(random_wait)\n",
    "    \n",
    "    page_values = [item['value'] for item in soup.find('select', {'name':'page', 'style':'padding: 1px; width: 204px;'}).find_all('option', {'value' : re.compile('^[0-9]{1,2}$')})]\n",
    "    if len(page_values) > 1:\n",
    "        for page in page_values[1:]:\n",
    "            save_url = start_page + '/' + date + '&page=' + page\n",
    "            save_name = 'pcs_ranking_' + date + '_' + page + '.html'\n",
    "            response = requests.get(save_url, headers=user_agent)\n",
    "            page = response.text\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "            with open(os.path.join('rankings/',save_name), 'w') as file:\n",
    "                file.write(str(soup))\n",
    "            print(dt.datetime.now(),'Saved:',save_name,end='\\r')\n",
    "\n",
    "            random_wait = 2 + 1 * random.random()\n",
    "            time.sleep(random_wait)\n",
    "    else:\n",
    "        continue\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing riders' pages from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in 'rider/' directory...\n",
    "fname = 'aaron-olson_2000.html'\n",
    "year = fname.split('_')[1].split('.')[0]\n",
    "\n",
    "with open(os.path.join('riders/',fname), 'r') as file:\n",
    "        page = file.read()\n",
    "        soup = BeautifulSoup(page, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Race Resume \"table\" containing datapoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find('div', class_='rdrRes').find_all('div', class_='row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Race Names per Season**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of race names column: 1 \n",
      "\n",
      "['National TT Championships New Zealand (NC)']\n"
     ]
    }
   ],
   "source": [
    "race_links = [c.find_all(['a'])[0] for c in rows]\n",
    "race_names = [re.sub(r'(</?a[^>]*>|</?b[^>]*>)','',str(record)).encode('latin-1').decode('utf-8').strip() for record in race_links]\n",
    "print('length of race names column:',len(race_names),'\\n')\n",
    "pp.pprint(race_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE: Looks like there are 9 more records here compared to date, result, and points columns. Those are the stage race title rows that sit right above the stage race rollup rows..._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date per Race**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of race dates column: 1 \n",
      "\n",
      "['09.01']\n"
     ]
    }
   ],
   "source": [
    "race_dates = [date.text.replace(u'\\xa0', u'').strip() for date in soup.find_all('span', style='width: 70px; ')]\n",
    "print('length of race dates column:',len(race_dates),'\\n')\n",
    "print(race_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Race Result (Rank) per Race**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of results column: 1 \n",
      "\n",
      "['13'] \n",
      "\n",
      "new length of results column (removed DNS/DNF): 1 \n",
      "\n",
      "[13] \n",
      "\n",
      "results frequencies: Counter({13: 1}) \n",
      "\n",
      "number of top 10 finishes: 0\n",
      "number of top 3 finishes: 0\n"
     ]
    }
   ],
   "source": [
    "results = [result.text for result in soup.find_all('span', style='width: 50px; text-align: center; ')]\n",
    "print('length of results column:',len(results),'\\n')\n",
    "print(results,'\\n')\n",
    "results = [int(item) for item in results if item != 'DNS' if item != 'DNF']\n",
    "print('new length of results column (removed DNS/DNF):',len(results),'\\n')\n",
    "print(results,'\\n')\n",
    "result_counts = Counter(results)\n",
    "print('results frequencies:',result_counts,'\\n')\n",
    "\n",
    "top_10s = sum([result_counts[key] for key in result_counts.keys() if key <= 10])\n",
    "top_3s = sum([result_counts[key] for key in result_counts.keys() if key <= 3])\n",
    "print('number of top 10 finishes:',top_10s)\n",
    "print('number of top 3 finishes:',top_3s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE: Will need to handle DNS and DNF when aggregating results and points._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCS Points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of PCS points column: 93\n",
      "total PCS points: 82.0 \n",
      "\n",
      "[0, 0, 5.0, 0, 50.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.0, 0, 0, 5.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.0, 0, 4.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.0, 0]\n"
     ]
    }
   ],
   "source": [
    "pcs_points = [point.text.replace(u'\\xa0', u'') for point in soup.find_all('span', style='width: 80px;   ')][1::3]\n",
    "pcs_points = [0 if item == '' else float(item) for item in pcs_points]\n",
    "total_pcs_points = sum(pcs_points)\n",
    "\n",
    "print('length of PCS points column:',len(pcs_points))\n",
    "print('total PCS points:',total_pcs_points,'\\n')\n",
    "print(pcs_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UCI Points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of UCI points column: 93 [0, 0, 0, 0, 6.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6.0, 0, 0, 5.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4.0, 0, 6.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.4, 0]\n",
      "total UCI points: 28.4 \n",
      "\n",
      "[0, 0, 0, 0, 6.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6.0, 0, 0, 5.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4.0, 0, 6.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.4, 0]\n"
     ]
    }
   ],
   "source": [
    "uci_points = [point.text.replace(u'\\xa0', u'') for point in soup.find_all('span', style='width: 80px;   ')][2::3]\n",
    "uci_points = [0 if item == '' else float(item) for item in uci_points]\n",
    "total_uci_points = sum(uci_points)\n",
    "\n",
    "print('length of UCI points column:',len(uci_points),uci_points)\n",
    "print('total UCI points:',total_uci_points,'\\n')\n",
    "print(uci_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Stage Races**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stage races: 0 \n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "stage_race_dates = [daterange.text for daterange in soup.find_all('span', style='width: 190px; ')]\n",
    "print('number of stage races:',len(stage_race_dates),'\\n')\n",
    "print(stage_race_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Distance and Race Days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season distance (km): 15390.6\n",
      "season race days: 91\n"
     ]
    }
   ],
   "source": [
    "season_distance = soup.find('div', style='border-top: 1px solid #ccc; margin-top: 1px; text-indent: 690px; padding-top: 2px; ').find('b').text\n",
    "print('season distance (km):',season_distance)\n",
    "\n",
    "season_race_days = soup.find('div', style='border-top: 1px solid #ccc; margin-top: 1px; text-indent: 690px; padding-top: 2px; ').contents[1].strip().split()[2]\n",
    "print('season race days:',season_race_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE: Any date, rank, and points associated with distance = 0 on same row may be rollup or final values for stage races. May want to handle them by removing or combining/converting into a separate feature._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample of extracting all datapoints and loading into dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date of birth': '1982 July 16',\n",
      " 'distance': 15390.6,\n",
      " 'height': 1.84,\n",
      " 'lifetime points general classification': 1708,\n",
      " 'lifetime points one day races': 4283,\n",
      " 'lifetime points sprint': 7703,\n",
      " 'lifetime points time trial': 760,\n",
      " 'lifetime points total': 14454,\n",
      " 'nationality': 'Germany',\n",
      " 'pcs points': 1316,\n",
      " 'race days': 91,\n",
      " 'rider': 'André Greipel',\n",
      " 'seasons': 16,\n",
      " 'stage races': 9,\n",
      " 'team': 'Lotto Soudal',\n",
      " 'uci points': 1613,\n",
      " 'weight': 75}\n"
     ]
    }
   ],
   "source": [
    "rider_seasons = {}\n",
    "\n",
    "rider = str(soup.find('title').text.encode('latin-1').decode())\n",
    "team = soup.find('span', class_='red').text.encode('latin-1').decode()\n",
    "nation = soup.find_all('div')[16].find('a', class_='black').text\n",
    "dob = soup.find_all('div')[16].find('span').contents[1:4]\n",
    "dob_day = dob[0].strip()\n",
    "dob_month = dob[2].split(' ')[1]\n",
    "dob_year = dob[2].split(' ')[2]\n",
    "dob_str = dob_year + ' ' + dob_month + ' ' + dob_day\n",
    "height = soup.find(text='Weight:').findNext().contents[1].split()[0]\n",
    "weight = str(soup.select('div:nth-of-type(2) > div:nth-of-type(3) > div:nth-of-type(4) > div:nth-of-type(2) > span > span > span:nth-of-type(2)')[0]).split('<span>')[1].split(' ')[1]\n",
    "lt_pts_one_day = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[0].text)\n",
    "lt_pts_gc = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[1].text)\n",
    "lt_pts_tt = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[2].text)\n",
    "lt_pts_sprint = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[3].text)\n",
    "lt_total_points = lt_pts_one_day + lt_pts_gc + lt_pts_tt + lt_pts_sprint\n",
    "pct_lt_oneday_pts = safe_div(lt_pts_one_day, lt_total_points)\n",
    "pct_lt_gc_pts = safe_div(lt_pts_gc, lt_total_points)\n",
    "pct_lt_tt_pts = safe_div(lt_pts_tt, lt_total_points)\n",
    "pct_lt_sprint_pts = safe_div(lt_pts_sprint, lt_total_points)\n",
    "\n",
    "\n",
    "seasons = len([year.text for year in soup.find('ul', class_='horiztree').contents])\n",
    "season_distance = soup.find('div', style='border-top: 1px solid #ccc; margin-top: 1px; text-indent: 690px; padding-top: 2px; ').find('b').text\n",
    "season_race_days = soup.find('div', style='border-top: 1px solid #ccc; margin-top: 1px; text-indent: 690px; padding-top: 2px; ').contents[1].strip().split()[2]\n",
    "stage_race_dates = len([daterange.text for daterange in soup.find_all('span', style='width: 190px; ')])\n",
    "uci_points = [point.text.replace(u'\\xa0', u'') for point in soup.find_all('span', style='width: 80px;   ')][2::3]\n",
    "uci_points = [0 if item == '' else int(item) for item in uci_points]\n",
    "total_uci_points = sum(uci_points)\n",
    "pcs_points = [point.text.replace(u'\\xa0', u'') for point in soup.find_all('span', style='width: 80px;   ')][1::3]\n",
    "pcs_points = [0 if item == '' else int(item) for item in pcs_points]\n",
    "total_pcs_points = sum(pcs_points)\n",
    "\n",
    "rider_seasons['rider'] = rider\n",
    "rider_seasons['team'] = team\n",
    "rider_seasons['nationality'] = nation\n",
    "rider_seasons['date of birth'] = dob_str\n",
    "rider_seasons['height'] = float(height)\n",
    "rider_seasons['weight'] = float(weight)\n",
    "rider_seasons['lifetime points one day races'] = lt_pts_one_day\n",
    "rider_seasons['lifetime points general classification'] = lt_pts_gc\n",
    "rider_seasons['lifetime points time trial'] = lt_pts_tt\n",
    "rider_seasons['lifetime points sprint'] = lt_pts_sprint\n",
    "rider_seasons['lifetime points total'] = lt_total_points\n",
    "rider_seasons['seasons'] = seasons\n",
    "rider_seasons['distance'] = float(season_distance)\n",
    "rider_seasons['race days'] = int(season_race_days)\n",
    "rider_seasons['stage races'] = len(stage_race_dates)\n",
    "rider_seasons['uci points'] = total_uci_points\n",
    "rider_seasons['pcs points'] = total_pcs_points\n",
    "\n",
    "pp.pprint(rider_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For now let's only pull the total PCS points per season for each rider and load into dictionary\n",
    "How well can we predict PCS points won in the most recent season using past seasons' points?  \n",
    "\n",
    "**Assumptions:**\n",
    "- PCS and UCI points are based on finishing results in each race\n",
    "- PCS awards points deeper down the results list than UCI\n",
    "- UCI awards more points to Grand Tours / top tier stage races\n",
    "\n",
    "**Hypothesis:**\n",
    "- A rider's performance in the next season is correlated with past seasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_div(n, d):\n",
    "    return n / d if d else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added Zico Waeytens season 2017  ---  1730 riders totaltotalaltotal total\r"
     ]
    }
   ],
   "source": [
    "all_riders_points = []\n",
    "rider_seasons = {}\n",
    "\n",
    "for file in sorted(glob.glob('riders/*.html')):\n",
    "    season = file.split('.')[0].split('_')[1]\n",
    "    \n",
    "    # skip 2018 since it's incomplete\n",
    "    if season != '2018':\n",
    "        with open(file, 'r') as f:\n",
    "            page = f.read()\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "            \n",
    "            rider = str(soup.find('title').text.encode('latin-1').decode())\n",
    "            seasons = [year.text for year in soup.find('ul', class_='horiztree').contents if year.text != '2018']\n",
    "            relative_season = seasons.index(season)\n",
    "            dob_year = soup.find('div', style='width: 230px; float: left; font: 12px/15px tahoma; ').find('span').contents[3].split()[1]\n",
    "            age = int(season) - int(dob_year)\n",
    "            nation = soup.find('a', class_='black').text\n",
    "            try:\n",
    "                height = float(soup.find(text='Weight:').findNext().contents[1].split()[0])\n",
    "            except:\n",
    "                height = float('nan')\n",
    "            try:\n",
    "                weight = float(str(soup.select('div:nth-of-type(2) > div:nth-of-type(3) > div:nth-of-type(4) > div:nth-of-type(2) > span > span > span:nth-of-type(2)')[0]).split('<span>')[1].split(' ')[1])\n",
    "            except:\n",
    "                weight = float('nan')\n",
    "            \n",
    "            pcs_points = [pcspoint.text.replace(u'\\xa0', u'') for pcspoint in soup.find_all('span', style='width: 80px;   ')][1::3]\n",
    "            pcs_points = [0 if item == '' else float(item) for item in pcs_points]\n",
    "            total_pcs_points = sum(pcs_points)\n",
    "            uci_points = [ucipoint.text.replace(u'\\xa0', u'') for ucipoint in soup.find_all('span', style='width: 80px;   ')][2::3]\n",
    "            uci_points = [0 if item == '' else float(item) for item in uci_points]\n",
    "            total_uci_points = sum(uci_points)\n",
    "            \n",
    "            lt_pts_one_day = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[0].text)\n",
    "            lt_pts_gc = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[1].text)\n",
    "            lt_pts_tt = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[2].text)\n",
    "            lt_pts_sprint = int(soup.find_all('div', style='display: inline-block; float: left; width: 23px; font: 9px/8px tahoma; color: #77B5C9; text-align: center;  ')[3].text)\n",
    "            lt_total_points = lt_pts_one_day + lt_pts_gc + lt_pts_tt + lt_pts_sprint\n",
    "            pct_lt_oneday_pts = safe_div(lt_pts_one_day, lt_total_points)\n",
    "            pct_lt_gc_pts = safe_div(lt_pts_gc, lt_total_points)\n",
    "            pct_lt_tt_pts = safe_div(lt_pts_tt, lt_total_points)\n",
    "            pct_lt_sprint_pts = safe_div(lt_pts_sprint, lt_total_points)\n",
    "            \n",
    "            if 'rider' not in rider_seasons:\n",
    "                rider_seasons['rider'] = rider\n",
    "                rider_seasons['nationality'] = nation\n",
    "                rider_seasons['height'] = height\n",
    "                rider_seasons['weight'] = weight\n",
    "                rider_seasons['seasons'] = len(seasons)\n",
    "                rider_seasons['pct_oneday_pts'] = pct_lt_oneday_pts\n",
    "                rider_seasons['pct_gc_pts'] = pct_lt_gc_pts\n",
    "                rider_seasons['pct_tt_pts'] = pct_lt_tt_pts\n",
    "                rider_seasons['pct_sprint_pts'] = pct_lt_sprint_pts\n",
    "            rider_seasons['S' + str(relative_season) + ' pcs pts'] = total_pcs_points\n",
    "            rider_seasons['S' + str(relative_season) + ' uci pts'] = total_uci_points\n",
    "            rider_seasons['S' + str(relative_season) + ' age'] = age\n",
    "            \n",
    "            if season == seasons[0]:\n",
    "                all_riders_points.append(rider_seasons.copy())\n",
    "                rider_seasons.clear()\n",
    "                print('added',rider,'season',season,' --- ',len(all_riders_points),'riders total',end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickle the list of dictionaries for modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('riders_relative_seasons.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_riders_points, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
